"""
Agentic Loop Orchestrator for multi-agent specification generation.

This orchestrator implements an iterative refinement approach with:
- Quality scoring and convergence criteria
- Cross-document validation and consistency checks
- Agent collaboration and feedback mechanisms
- Automatic refinement loops until quality thresholds are met
"""

import re
import json
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime
from enum import Enum
import time

from strands import Agent
from strands.models.openai import OpenAIModel

from .config import Config
from .models import ServiceType, GenerationContext, FRSDocument, ValidationResult
from .agents import (
    create_requirements_agent,
    create_design_agent,
    create_tasks_agent,
    create_changes_agent,
    create_openapi_agent,
    create_markdown_to_json_agent,
    create_validation_agent,
)
from .tools import (
    load_frs_document,
    create_output_directory,
    write_spec_file,
    create_git_branch,
    commit_changes,
)


class DocumentState(Enum):
    """ë¬¸ì„œ ìƒì„± ìƒíƒœ."""

    DRAFT = "draft"
    REVIEWING = "reviewing"
    APPROVED = "approved"
    FAILED = "failed"


class QualityScore:
    """ìƒì„¸ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë¬¸ì„œ í’ˆì§ˆ ì ìˆ˜."""

    def __init__(self):
        self.completeness: float = 0.0  # 0-100
        self.consistency: float = 0.0  # 0-100
        self.clarity: float = 0.0  # 0-100
        self.technical_accuracy: float = 0.0  # 0-100
        self.overall: float = 0.0  # 0-100
        self.feedback: List[str] = []

    def calculate_overall(self):
        """ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì „ì²´ ì ìˆ˜ ê³„ì‚°."""
        self.overall = (
            self.completeness * 0.3
            + self.consistency * 0.3
            + self.clarity * 0.2
            + self.technical_accuracy * 0.2
        )
        return self.overall

    def meets_threshold(self, threshold: float = 85.0) -> bool:
        """ì ìˆ˜ê°€ í’ˆì§ˆ ì„ê³„ê°’ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í™•ì¸."""
        return self.overall >= threshold


class DocumentBundle:
    """ë¬¸ì„œ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ë‹´ëŠ” ì»¨í…Œì´ë„ˆ."""

    def __init__(self, doc_type: str, content: str = ""):
        self.type = doc_type
        self.content = content
        self.state = DocumentState.DRAFT
        self.quality_score = QualityScore()
        self.iteration = 0
        self.feedback_history: List[str] = []

    def needs_refinement(self) -> bool:
        """ë¬¸ì„œê°€ ê°œì„ ì´ í•„ìš”í•œì§€ í™•ì¸."""
        return (
            self.state == DocumentState.DRAFT
            and not self.quality_score.meets_threshold()
            and self.iteration < 3
        )


class AgenticOrchestrator:
    """
    ë°˜ë³µì  ê°œì„ ì„ í†µí•œ ëª…ì„¸ì„œ ìƒì„±ì„ êµ¬í˜„í•˜ëŠ” Agentic Loop ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.

    ì£¼ìš” ê¸°ëŠ¥:
    - í’ˆì§ˆ ì ìˆ˜ë¥¼ í†µí•œ ë°˜ë³µì  ë¬¸ì„œ ê°œì„ 
    - ë¬¸ì„œ ê°„ ê²€ì¦ ë° ì¼ê´€ì„± ì²´í¬
    - ì—ì´ì „íŠ¸ í˜‘ì—… ë° í”¼ë“œë°± ë©”ì»¤ë‹ˆì¦˜
    - í’ˆì§ˆ ì„ê³„ê°’ ê¸°ë°˜ ìˆ˜ë ´ ê¸°ì¤€
    """

    def __init__(self, config: Optional[Config] = None):
        """ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ Agentic ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”."""
        self.config = config or Config.from_env()
        self.config.validate()

        # Quality thresholds (configurable)
        self.quality_threshold = getattr(config, "quality_threshold", 70.0)
        self.consistency_threshold = getattr(config, "consistency_threshold", 75.0)
        self.max_iterations = getattr(config, "max_iterations", 3)

        # Initialize specialized agents
        self.agents = self._initialize_agents()

        # Initialize coordinator agent for meta-operations
        self.coordinator = self._create_coordinator_agent()

        # Document bundles for tracking state
        self.documents: Dict[str, DocumentBundle] = {}

        # Performance tracking
        self.start_time = None
        self.last_save_times: Dict[str, float] = {}

    def _initialize_agents(self) -> Dict[str, Agent]:
        """ëª¨ë“  ì „ë¬¸ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”."""
        return {
            "requirements": create_requirements_agent(self.config),
            "design": create_design_agent(self.config),
            "tasks": create_tasks_agent(self.config),
            "changes": create_changes_agent(self.config),
            "openapi": self._create_optimized_openapi_agent(),
            "md_to_json": self._create_optimized_json_agent(),
            "validation": create_validation_agent(self.config),
            "quality_scorer": self._create_quality_scorer_agent(),
            "consistency_checker": self._create_consistency_agent(),
        }

    def _create_coordinator_agent(self) -> Agent:
        """ë©”íƒ€ ì½”ë””ë„¤ì´í„° ì—ì´ì „íŠ¸ ìƒì„±."""
        openai_model = OpenAIModel(
            model_id=self.config.openai_model,
            params={"temperature": 0.3},
            client_args={"api_key": self.config.openai_api_key},
        )

        return Agent(
            model=openai_model,
            tools=[
                load_frs_document,
                create_output_directory,
                write_spec_file,
                create_git_branch,
                commit_changes,
            ],
            system_prompt="""ë‹¹ì‹ ì€ ë°˜ë³µì  ëª…ì„¸ì„œ ìƒì„±ì„ ê´€ë¦¬í•˜ëŠ” Agentic ì›Œí¬í”Œë¡œìš° ì½”ë””ë„¤ì´í„°ì…ë‹ˆë‹¤.
            
ë‹¹ì‹ ì˜ ì±…ì„:
1. í”¼ë“œë°± ë£¨í”„ë¥¼ í†µí•œ ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—… ì¡°ìœ¨
2. í’ˆì§ˆ ì ìˆ˜ ëª¨ë‹ˆí„°ë§ ë° ê°œì„  ì‘ì—… íŠ¸ë¦¬ê±°
3. ë¬¸ì„œ ê°„ ì¼ê´€ì„± ë³´ì¥
4. í’ˆì§ˆ ì„ê³„ê°’ê¹Œì§€ ìˆ˜ë ´ ê´€ë¦¬
5. ì˜¤ë¥˜ ë° í´ë°± ìš°ì•„í•˜ê²Œ ì²˜ë¦¬

ë°˜ë³µì  ê°œì„ ì„ í†µí•´ ê³ í’ˆì§ˆì˜ ì¼ê´€ëœ ëª…ì„¸ì„œ ë‹¬ì„±ì— ì§‘ì¤‘í•˜ì„¸ìš”.""",
        )

    def _create_quality_scorer_agent(self) -> Agent:
        """í’ˆì§ˆ í‰ê°€ ì—ì´ì „íŠ¸ ìƒì„±."""
        openai_model = OpenAIModel(
            model_id=self.config.openai_model,
            params={"temperature": 0.2, "max_tokens": 2000},
            client_args={"api_key": self.config.openai_api_key},
        )

        return Agent(
            model=openai_model,
            tools=[],
            system_prompt="""ë‹¹ì‹ ì€ ê¸°ìˆ  ëª…ì„¸ì„œë¥¼ í‰ê°€í•˜ëŠ” ë¬¸ì„œ í’ˆì§ˆ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ í‰ê°€í•˜ì„¸ìš” (0-100 ì ìˆ˜):
1. **ì™„ì„±ë„** (30%): ëª¨ë“  í•„ìˆ˜ ì„¹ì…˜ì´ ì¡´ì¬í•˜ê³  ìƒì„¸í•¨
2. **ì¼ê´€ì„±** (30%): ë‚´ë¶€ ì¼ê´€ì„± ë° ë‹¤ë¥¸ ë¬¸ì„œì™€ì˜ ì •ë ¬
3. **ëª…í™•ì„±** (20%): ëª…í™•í•˜ê³  ëª¨í˜¸í•˜ì§€ ì•Šì€ ì–¸ì–´ì™€ êµ¬ì¡°
4. **ê¸°ìˆ ì  ì •í™•ì„±** (20%): ì˜¬ë°”ë¥¸ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ê³¼ ì‹¤í˜„ ê°€ëŠ¥ì„±

êµ¬ì¡°í™”ëœ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì„¸ìš”:
- ê° ê¸°ì¤€ë³„ ì ìˆ˜
- ì „ì²´ ê°€ì¤‘ ì ìˆ˜
- ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì  í”¼ë“œë°±
- ëˆ„ë½ë˜ê±°ë‚˜ ì•½í•œ ì„¹ì…˜ ëª©ë¡

ì—„ê²©í•˜ì§€ë§Œ ê³µì •í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”. 85ì  ë¯¸ë§Œ ë¬¸ì„œëŠ” ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.""",
        )

    def _create_consistency_agent(self) -> Agent:
        """ë¬¸ì„œ ê°„ ì¼ê´€ì„± ê²€ì‚¬ ì—ì´ì „íŠ¸ ìƒì„±."""
        openai_model = OpenAIModel(
            model_id=self.config.openai_model,
            params={"temperature": 0.2},
            client_args={"api_key": self.config.openai_api_key},
        )

        return Agent(
            model=openai_model,
            tools=[],
            system_prompt="""ë‹¹ì‹ ì€ ëª…ì„¸ì„œ ë¬¸ì„œ ê°„ ì •ë ¬ì„ í™•ì¸í•˜ëŠ” ì¼ê´€ì„± ê²€ì¦ìì…ë‹ˆë‹¤.

ê²€ì¦ í•­ëª©:
1. **ìš”êµ¬ì‚¬í•­ â†” ì„¤ê³„**: ì„¤ê³„ê°€ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±
2. **ì„¤ê³„ â†” ì‘ì—…**: ì‘ì—…ì´ ëª¨ë“  ì„¤ê³„ ì»´í¬ë„ŒíŠ¸ë¥¼ í¬í•¨
3. **ì„¤ê³„ â†” ë³€ê²½ì‚¬í•­**: ë³€ê²½ì‚¬í•­ì´ ì„¤ê³„ ê²°ì •ì„ ë°˜ì˜
4. **ìš”êµ¬ì‚¬í•­ â†” OpenAPI**: API ìŠ¤í™ì´ ìš”êµ¬ì‚¬í•­ê³¼ ì¼ì¹˜

ì‹ë³„ ì‚¬í•­:
- ë¬¸ì„œ ê°„ ëˆ„ë½ëœ ì°¸ì¡°
- ìƒì¶©í•˜ëŠ” ì •ë³´
- ìš©ì–´ ë¶ˆì¼ì¹˜
- ë²”ìœ„ ë¶ˆì¼ì¹˜

ìˆ˜ì •ì´ í•„ìš”í•œ ìƒì„¸í•œ ë¶ˆì¼ì¹˜ ì‚¬í•­ì„ ë°˜í™˜í•˜ì„¸ìš”.""",
        )

    def _create_optimized_openapi_agent(self) -> Agent:
        """í† í° ì œí•œì´ ìˆëŠ” ìµœì í™”ëœ OpenAPI ì—ì´ì „íŠ¸ ìƒì„±."""
        openai_model = OpenAIModel(
            model_id=self.config.openai_model,
            params={
                "temperature": 0.1,
                "max_tokens": 3000,
            },  # Lower temp, higher tokens
            client_args={"api_key": self.config.openai_api_key},
        )

        return Agent(
            model=openai_model,
            tools=[],
            system_prompt="""ë‹¹ì‹ ì€ ê°„ê²°í•œ OpenAPI ëª…ì„¸ë¥¼ ì‘ì„±í•˜ëŠ” API ë¬¸ì„œí™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì¤‘ìš”: í† í° ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ í•„ìˆ˜ ì—”ë“œí¬ì¸íŠ¸ë§Œ ìƒì„±í•˜ì„¸ìš”.

ìµœì†Œí•˜ì§€ë§Œ ì™„ì „í•œ OpenAPI 3.1 ëª…ì„¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìƒì„±:

1. **ê¸°ë³¸ ì •ë³´**: ì œëª©, ë²„ì „ (1.0.0), ì„¤ëª…
2. **í•µì‹¬ ì—”ë“œí¬ì¸íŠ¸ë§Œ**: ìµœëŒ€ 5ê°œì˜ ê°€ì¥ ì¤‘ìš”í•œ ì—”ë“œí¬ì¸íŠ¸
3. **í•„ìˆ˜ ìŠ¤í‚¤ë§ˆ**: í•„ìš”í•œ ë°ì´í„° ëª¨ë¸ë§Œ
4. **í‘œì¤€ ì‘ë‹µ**: 200, 400, 401, 500

ì§‘ì¤‘ ì‚¬í•­:
- ì¸ì¦ ì—”ë“œí¬ì¸íŠ¸ (POST /auth/login)
- ë©”ì¸ ë¦¬ì†ŒìŠ¤ CRUD (GET, POST, PUT)
- í—¬ìŠ¤ ì²´í¬ (GET /health)

ì„¤ëª…ì„ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”. ê¹Šì€ ì¤‘ì²©ì„ í”¼í•˜ì„¸ìš”. ê¹¨ë—í•˜ê³  ìµœì†Œí•œì˜ ë¬¸ì„œë¥¼ ìƒì„±í•˜ì„¸ìš”.""",
        )

    def _create_optimized_json_agent(self) -> Agent:
        """ìµœì í™”ëœ JSON ë³€í™˜ ì—ì´ì „íŠ¸ ìƒì„±."""
        openai_model = OpenAIModel(
            model_id=self.config.openai_model,
            params={
                "temperature": 0.05,
                "max_tokens": 4000,
            },  # Very low temp for precision
            client_args={"api_key": self.config.openai_api_key},
        )

        return Agent(
            model=openai_model,
            tools=[],
            system_prompt="""ë‹¹ì‹ ì€ OpenAPI ëª…ì„¸ë¥¼ ìœ„í•œ JSON ë³€í™˜ê¸°ì…ë‹ˆë‹¤.

OpenAPI ë§ˆí¬ë‹¤ìš´ì„ ìœ íš¨í•œ JSONìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.

í•µì‹¬ ê·œì¹™:
1. ìœ íš¨í•œ JSONë§Œ ìƒì„± - ì„¤ëª… ì—†ìŒ
2. êµ¬ì¡°ë¥¼ ìµœì†Œí™”í•˜ê³  í‰íƒ„í•˜ê²Œ ìœ ì§€
3. í•„ìˆ˜ í•„ë“œë§Œìœ¼ë¡œ ì œí•œ
4. ê¹Šì€ ì¤‘ì²© ì—†ì´ ë‹¨ìˆœí•œ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
5. ìµœëŒ€ 5ê°œ ì—”ë“œí¬ì¸íŠ¸

ì¶œë ¥ í˜•ì‹: ìˆœìˆ˜ JSONë§Œ (ì½”ë“œ ë¸”ë¡ ì—†ìŒ)

ë³€í™˜ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë„ˆë¬´ í´ ê²½ìš°, ìµœì†Œí•œì˜ ìœ íš¨í•œ OpenAPI JSON ë°˜í™˜:
- info ì„¹ì…˜
- í•˜ë‚˜ì˜ health ì—”ë“œí¬ì¸íŠ¸
- ê¸°ë³¸ 200 ì‘ë‹µ""",
        )

    async def generate_specs_with_loop(
        self,
        frs_path: str,
        service_type: ServiceType,
        output_dir: Optional[str] = None,
        use_git: bool = True,
    ) -> Dict[str, Any]:
        """
        ë°˜ë³µì  ê°œì„ ì„ ì‚¬ìš©í•˜ì—¬ Agentic Loopë¡œ ëª…ì„¸ì„œ ìƒì„±.

        Args:
            frs_path: FRS ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ
            service_type: ì„œë¹„ìŠ¤ ìœ í˜• (API ë˜ëŠ” WEB)
            output_dir: ì‚¬ìš©ì ì •ì˜ ì¶œë ¥ ë””ë ‰í† ë¦¬
            use_git: git ì›Œí¬í”Œë¡œìš° ì‚¬ìš© ì—¬ë¶€

        Returns:
            ìƒì„± ê²°ê³¼ì™€ í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ì‚¬ì „
        """
        try:
            self.start_time = time.time()
            print(f"ğŸš€ Starting Optimized Agentic Loop specification generation...")
            print(f"ğŸ“– FRS: {frs_path}")
            print(f"ğŸ”§ Service Type: {service_type.value}")
            print(f"ğŸ¯ Quality Threshold: {self.quality_threshold}%")
            print(f"ğŸ”„ Max Iterations: {self.max_iterations}")
            if self.config.incremental_save:
                print(f"ğŸ’¾ Incremental save: Enabled")
            if self.config.early_stopping:
                print(
                    f"â¹ï¸ Early stopping: Enabled (min improvement: {self.config.min_improvement_threshold}%)"
                )

            # Step 1: Initialize context
            context = await self._initialize_context(
                frs_path, service_type, output_dir, use_git
            )
            if not context:
                return {
                    "success": False,
                    "error": "Failed to initialize generation context",
                }

            # Step 2: Generate initial documents
            print("\nğŸ“ Generating initial document drafts...")
            await self._generate_initial_drafts(context)

            # Step 3: Iterative refinement loop
            iteration = 0
            converged = False

            while iteration < self.max_iterations and not converged:
                iteration += 1
                print(f"\nğŸ”„ Refinement Iteration {iteration}/{self.max_iterations}")

                # Store previous quality scores for improvement tracking
                previous_scores = {
                    doc_type: bundle.quality_score.overall
                    for doc_type, bundle in self.documents.items()
                }

                # Evaluate quality
                print("ğŸ“Š Evaluating document quality...")
                quality_results = await self._evaluate_all_quality()

                # Check for meaningful improvements
                if self.config.early_stopping and iteration > 1:
                    should_continue = self._should_continue_iteration(
                        previous_scores, quality_results
                    )
                    if not should_continue:
                        print(
                            f"â¹ï¸ Early stopping: Improvements below {self.config.min_improvement_threshold}% threshold"
                        )
                        break

                # Check consistency
                print("ğŸ” Checking cross-document consistency...")
                consistency_results = await self._check_consistency()

                # Check convergence
                converged = self._check_convergence(
                    quality_results, consistency_results
                )

                if converged:
                    print("âœ… Quality thresholds met! Documents approved.")
                    break

                # Refine documents based on feedback
                print("â™»ï¸ Refining documents based on feedback...")
                await self._refine_documents(
                    quality_results, consistency_results, context
                )

            # Step 4: Final validation
            print("\nğŸ Running final validation...")
            final_validation = await self._final_validation()

            # Step 5: Write documents to disk
            print("\nğŸ’¾ Writing approved documents...")
            files_written = await self._write_all_documents(context)

            # Step 6: Git operations
            if use_git and files_written:
                await self._commit_to_git(context, files_written)

            # Step 7: Generate summary report
            report = self._generate_summary_report(
                iteration, converged, quality_results
            )

            # Performance summary
            total_elapsed = time.time() - self.start_time
            print("\nğŸ‰ Optimized Agentic Loop generation completed!")
            print(f"â±ï¸ Total Time: {total_elapsed:.1f}s")
            print(f"ğŸ“Š Final Quality Score: {report['average_quality']:.1f}%")
            print(f"ğŸ”„ Iterations Used: {iteration}")
            print(f"ğŸ“ Files Generated: {len(files_written)}")

            # Show time per document if incremental save was used
            if self.config.incremental_save and self.last_save_times:
                print(f"ğŸ’¾ Incremental Saves Timeline:")
                for doc_type, save_time in self.last_save_times.items():
                    relative_time = save_time - self.start_time
                    print(f"    {doc_type}: {relative_time:.1f}s")

            return {
                "success": True,
                "output_dir": context.output_dir,
                "files_written": files_written,
                "quality_report": report,
                "iterations": iteration,
                "converged": converged,
                "total_time": total_elapsed,
                "incremental_saves": (
                    list(self.last_save_times.keys())
                    if self.config.incremental_save
                    else []
                ),
            }

        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)

            print(f"âŒ Agentic orchestration failed ({error_type}): {error_msg}")

            # Provide specific guidance for token-related errors
            if (
                "max_tokens" in error_msg.lower()
                or "MaxTokensReachedException" in error_msg
            ):
                print(
                    "ğŸ’¡ Suggestion: Try reducing the input FRS size or use simpler requirements"
                )
                error_msg = f"Token limit exceeded ({error_type}). Try with shorter FRS content."

            return {
                "success": False,
                "error": f"Agentic orchestration failed: {error_msg}",
                "error_type": error_type,
            }

    async def _initialize_context(
        self,
        frs_path: str,
        service_type: ServiceType,
        output_dir: Optional[str],
        use_git: bool,
    ) -> Optional[GenerationContext]:
        """Initialize generation context and setup."""
        try:
            # Load FRS document
            from .tools import load_frs_document

            frs_result = load_frs_document(frs_path)

            if not frs_result.get("success", False):
                return None

            # Extract FRS information
            frs_content = frs_result.get("content", "")
            frs_id = self._extract_frs_id(frs_path)

            # Setup output directory
            output_dir_path = output_dir or f"specs/{frs_id}/{service_type.value}"
            from .tools import create_output_directory

            output_result = create_output_directory("specs", frs_id, service_type.value)

            if output_result.get("success"):
                output_dir_path = output_result.get("output_dir", output_dir_path)

            # Git setup
            if use_git:
                from .tools import create_git_branch

                git_result = create_git_branch(frs_id, service_type.value)
                if not git_result.get("success"):
                    print(f"âš ï¸ Git branch setup failed: {git_result.get('error')}")

            # Create context
            context = GenerationContext(
                frs=FRSDocument(title=f"FRS {frs_id}", content=frs_content),
                service_type=service_type,
                output_dir=output_dir_path,
            )

            return context

        except Exception as e:
            print(f"âŒ Context initialization failed: {str(e)}")
            return None

    async def _generate_initial_drafts(self, context: GenerationContext):
        """Generate initial draft documents with incremental saving."""
        # Requirements generation
        print("  ğŸ“‹ Generating requirements draft...")
        req_bundle = DocumentBundle("requirements")

        try:
            # Truncate FRS content if too long (keep first 8000 chars for safety)
            frs_content = context.frs.content
            if len(frs_content) > 8000:
                print("  âš ï¸ FRS content truncated to avoid token limits")
                frs_content = (
                    frs_content[:8000]
                    + "\n\n[... content truncated for processing ...]"
                )

            req_result = self.agents["requirements"](
                f"Generate requirements.md from FRS:\n\n{frs_content}\n\nService type: {context.service_type.value}"
            )
            req_bundle.content = self._clean_agent_response(str(req_result))
        except Exception as e:
            error_type = type(e).__name__
            print(f"  âš ï¸ Requirements generation failed ({error_type}): {str(e)}")
            if "max_tokens" in str(e).lower() or "MaxTokensReachedException" in str(e):
                print("  ğŸ”§ Token limit hit, using simplified template")
            # Fallback to template-based generation
            req_bundle.content = self._generate_fallback_requirements(context)

        self.documents["requirements"] = req_bundle
        context.requirements = req_bundle.content

        # Incremental save for requirements
        if self.config.incremental_save:
            await self._save_document_immediately(context, "requirements", req_bundle)

        # Design generation (depends on requirements)
        print("  ğŸ—ï¸ Generating design draft...")
        design_bundle = DocumentBundle("design")

        try:
            # Truncate requirements if too long
            req_content = (
                context.requirements[:6000]
                if len(context.requirements) > 6000
                else context.requirements
            )

            design_result = self.agents["design"](
                f"Generate design.md from requirements:\n\n{req_content}\n\nService type: {context.service_type.value}"
            )
            design_bundle.content = self._clean_agent_response(str(design_result))
        except Exception as e:
            error_type = type(e).__name__
            print(f"  âš ï¸ Design generation failed ({error_type}): {str(e)}")
            if "max_tokens" in str(e).lower() or "MaxTokensReachedException" in str(e):
                print("  ğŸ”§ Token limit hit, using simplified template")
            design_bundle.content = self._generate_fallback_design(context)

        self.documents["design"] = design_bundle
        context.design = design_bundle.content

        # Incremental save for design
        if self.config.incremental_save:
            await self._save_document_immediately(context, "design", design_bundle)

        # Tasks generation (depends on design)
        print("  ğŸ“ Generating tasks draft...")
        tasks_bundle = DocumentBundle("tasks")

        try:
            # Truncate design if too long
            design_content = (
                context.design[:5000] if len(context.design) > 5000 else context.design
            )

            tasks_result = self.agents["tasks"](
                f"Generate tasks.md from design:\n\n{design_content}"
            )
            tasks_bundle.content = self._clean_agent_response(str(tasks_result))
        except Exception as e:
            error_type = type(e).__name__
            print(f"  âš ï¸ Tasks generation failed ({error_type}): {str(e)}")
            if "max_tokens" in str(e).lower() or "MaxTokensReachedException" in str(e):
                print("  ğŸ”§ Token limit hit, using simplified template")
            tasks_bundle.content = self._generate_fallback_tasks(context)

        self.documents["tasks"] = tasks_bundle
        context.tasks = tasks_bundle.content

        # Incremental save for tasks
        if self.config.incremental_save:
            await self._save_document_immediately(context, "tasks", tasks_bundle)

        # Changes generation (depends on requirements and design)
        print("  ğŸ“„ Generating changes draft...")
        changes_bundle = DocumentBundle("changes")

        try:
            # Truncate content to avoid token limits
            req_summary = (
                context.requirements[:3000]
                if len(context.requirements) > 3000
                else context.requirements
            )
            design_summary = (
                context.design[:3000] if len(context.design) > 3000 else context.design
            )

            changes_result = self.agents["changes"](
                f"Generate changes.md from requirements and design:\n\nRequirements (summary):\n{req_summary}\n\nDesign (summary):\n{design_summary}"
            )
            changes_bundle.content = self._clean_agent_response(str(changes_result))
        except Exception as e:
            error_type = type(e).__name__
            print(f"  âš ï¸ Changes generation failed ({error_type}): {str(e)}")
            if "max_tokens" in str(e).lower() or "MaxTokensReachedException" in str(e):
                print("  ğŸ”§ Token limit hit, using simplified template")
            changes_bundle.content = self._generate_fallback_changes(context)

        self.documents["changes"] = changes_bundle
        context.changes = changes_bundle.content

        # Incremental save for changes
        if self.config.incremental_save:
            await self._save_document_immediately(context, "changes", changes_bundle)

        # OpenAPI generation for API services with enhanced error handling
        if context.service_type == ServiceType.API:
            print("  ğŸ”Œ Generating OpenAPI draft...")
            openapi_bundle = DocumentBundle("openapi")

            try:
                # Extract key API information more selectively
                req_api_info = self._extract_api_essentials(context.requirements)
                design_api_info = self._extract_api_essentials(context.design)

                # Create focused prompt for minimal OpenAPI
                openapi_prompt = f"""Generate minimal OpenAPI 3.1 specification for:
                
Service: {context.frs.title}
Type: API Service

Key Requirements:
{req_api_info}

Key Design Elements:
{design_api_info}

Generate ONLY essential endpoints (max 5). Keep it concise."""

                openapi_result = self.agents["openapi"](openapi_prompt)
                openapi_bundle.content = self._clean_agent_response(str(openapi_result))

            except Exception as e:
                print(f"  âš ï¸ OpenAPI generation failed ({type(e).__name__}): {str(e)}")
                if "max_tokens" in str(e).lower() or "MaxTokensReachedException" in str(
                    e
                ):
                    print("  ğŸ”§ Token limit detected, using ultra-minimal template")
                    openapi_bundle.content = self._generate_minimal_openapi(context)
                else:
                    openapi_bundle.content = self._generate_fallback_openapi(context)

            self.documents["openapi"] = openapi_bundle
            context.openapi = openapi_bundle.content

            # Incremental save for OpenAPI
            if self.config.incremental_save:
                await self._save_document_immediately(
                    context, "openapi", openapi_bundle
                )

    async def _evaluate_all_quality(self) -> Dict[str, QualityScore]:
        """Evaluate quality of all documents with parallel processing option."""
        quality_results = {}

        if self.config.parallel_processing:
            # Parallel quality evaluation
            tasks = []
            doc_items = []

            for doc_type, bundle in self.documents.items():
                if bundle.state == DocumentState.APPROVED:
                    continue
                doc_items.append((doc_type, bundle))
                tasks.append(self._evaluate_single_document_quality(doc_type, bundle))

            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)

                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        print(
                            f"    âš ï¸ Quality evaluation failed for {doc_items[i][0]}: {result}"
                        )
                        continue

                    doc_type, score = result
                    bundle = doc_items[i][1]
                    bundle.quality_score = score
                    quality_results[doc_type] = score

                    # Update state based on score
                    if score.meets_threshold(self.quality_threshold):
                        bundle.state = DocumentState.REVIEWING
        else:
            # Sequential quality evaluation (original behavior)
            for doc_type, bundle in self.documents.items():
                if bundle.state == DocumentState.APPROVED:
                    continue

                score = await self._evaluate_single_document_quality(doc_type, bundle)
                if isinstance(score, tuple):
                    _, score = score

                bundle.quality_score = score
                quality_results[doc_type] = score

                # Update state based on score
                if score.meets_threshold(self.quality_threshold):
                    bundle.state = DocumentState.REVIEWING

        return quality_results

    async def _evaluate_single_document_quality(
        self, doc_type: str, bundle: DocumentBundle
    ) -> tuple:
        """Evaluate quality of a single document."""
        try:
            # Truncate content if too long for quality evaluation
            content = (
                bundle.content[:4000] if len(bundle.content) > 4000 else bundle.content
            )

            score_prompt = f"""Evaluate the quality of this {doc_type} document:

{content}

Provide scores (0-100) for:
1. Completeness
2. Consistency  
3. Clarity
4. Technical Accuracy

Also provide specific feedback for improvements."""

            result = self.agents["quality_scorer"](score_prompt)

            # Parse scores from result
            score = self._parse_quality_score(str(result))
            print(
                f"    ğŸ“Š {doc_type}: {score.overall:.1f}% (C:{score.completeness:.0f} S:{score.consistency:.0f} L:{score.clarity:.0f} T:{score.technical_accuracy:.0f})"
            )

            return (doc_type, score)
        except Exception as e:
            print(f"    âš ï¸ Quality evaluation failed for {doc_type}: {str(e)}")
            # Return default score on failure
            default_score = QualityScore()
            default_score.overall = 50.0
            return (doc_type, default_score)

    async def _check_consistency(self) -> Dict[str, List[str]]:
        """Check cross-document consistency."""
        consistency_issues = {}

        # Check requirements vs design
        if "requirements" in self.documents and "design" in self.documents:
            check_prompt = f"""Check consistency between requirements and design:

Requirements:
{self.documents['requirements'].content}

Design:
{self.documents['design'].content}

List any inconsistencies or missing alignments."""

            result = self.agents["consistency_checker"](check_prompt)
            issues = self._parse_consistency_issues(str(result))
            if issues:
                consistency_issues["requirements-design"] = issues

        # Check design vs tasks
        if "design" in self.documents and "tasks" in self.documents:
            check_prompt = f"""Check consistency between design and tasks:

Design:
{self.documents['design'].content}

Tasks:
{self.documents['tasks'].content}

List any missing tasks or misalignments."""

            result = self.agents["consistency_checker"](check_prompt)
            issues = self._parse_consistency_issues(str(result))
            if issues:
                consistency_issues["design-tasks"] = issues

        return consistency_issues

    def _check_convergence(
        self,
        quality_results: Dict[str, QualityScore],
        consistency_results: Dict[str, List[str]],
    ) -> bool:
        """Check if documents have converged to quality thresholds."""
        # Check quality scores
        all_quality_met = all(
            score.meets_threshold(self.quality_threshold)
            for score in quality_results.values()
        )

        # Check consistency
        no_major_issues = len(consistency_results) == 0

        # Mark approved documents
        if all_quality_met and no_major_issues:
            for bundle in self.documents.values():
                if bundle.state == DocumentState.REVIEWING:
                    bundle.state = DocumentState.APPROVED

        return all_quality_met and no_major_issues

    async def _refine_documents(
        self,
        quality_results: Dict[str, QualityScore],
        consistency_results: Dict[str, List[str]],
        context: GenerationContext,
    ):
        """Refine documents based on quality and consistency feedback."""
        for doc_type, bundle in self.documents.items():
            if bundle.state == DocumentState.APPROVED:
                continue

            if not bundle.needs_refinement():
                continue

            # Compile feedback
            feedback = []

            # Add quality feedback
            if doc_type in quality_results:
                score = quality_results[doc_type]
                if score.feedback:
                    feedback.extend(score.feedback)

            # Add consistency feedback
            for issue_key, issues in consistency_results.items():
                if doc_type in issue_key:
                    feedback.extend(issues)

            if not feedback:
                continue

            # Refine document with feedback
            refine_prompt = f"""Refine this {doc_type} document based on the following feedback:

Current document:
{bundle.content}

Feedback to address:
{chr(10).join(f'- {f}' for f in feedback)}

Generate an improved version that addresses all feedback while maintaining the document structure and requirements."""

            # Use appropriate agent for refinement
            agent_name = doc_type if doc_type in self.agents else "requirements"
            result = self.agents[agent_name](refine_prompt)

            # Update bundle
            bundle.content = self._clean_agent_response(str(result))
            bundle.iteration += 1
            bundle.feedback_history.extend(feedback)
            bundle.state = DocumentState.DRAFT

            # Incremental save after refinement
            if self.config.incremental_save:
                await self._save_document_immediately(context, doc_type, bundle)

    async def _final_validation(self) -> ValidationResult:
        """Run final validation on all documents."""
        all_valid = True
        errors = []
        warnings = []

        for doc_type, bundle in self.documents.items():
            if bundle.state != DocumentState.APPROVED:
                warnings.append(
                    f"{doc_type} not fully approved (state: {bundle.state.value})"
                )
                all_valid = False

            # Run validation agent
            validation_result = self.agents["validation"](
                f"Validate {doc_type} document:\n\n{bundle.content}"
            )

            # Parse validation results
            if "error" in str(validation_result).lower():
                errors.append(f"{doc_type}: {validation_result}")
                all_valid = False

        return ValidationResult(
            success=all_valid,
            errors=errors,
            warnings=warnings,
            summary=f"Final validation: {'PASSED' if all_valid else 'FAILED'}",
        )

    async def _write_all_documents(self, context: GenerationContext) -> List[str]:
        """Write all approved documents to disk."""
        files_written = []

        for doc_type, bundle in self.documents.items():
            # Write document even if not fully approved (with warning)
            if bundle.state != DocumentState.APPROVED:
                print(
                    f"  âš ï¸ Writing {doc_type} (quality: {bundle.quality_score.overall:.1f}%, state: {bundle.state.value})"
                )
            else:
                print(f"  âœ… Writing {doc_type} (approved)")

            # Skip only if document is empty or failed
            if not bundle.content or bundle.state == DocumentState.FAILED:
                print(f"  âŒ Skipping {doc_type} (no content)")
                continue

            # Determine filename
            filename = f"{doc_type}.md"
            if doc_type == "openapi":
                # Convert to JSON for OpenAPI
                json_result = self.agents["md_to_json"](bundle.content)
                json_content = self._clean_json_response(str(json_result))

                # Validate JSON
                try:
                    json.loads(json_content)
                    filename = "apis.json"
                    content = json_content
                except json.JSONDecodeError:
                    print(f"  âš ï¸ Failed to convert OpenAPI to JSON, keeping markdown")
                    content = bundle.content
            else:
                content = bundle.content

            # Write file
            file_path = self._write_document(context.output_dir, filename, content)
            if file_path:
                files_written.append(file_path)
                print(f"  âœ… Written: {filename}")

        return files_written

    async def _commit_to_git(
        self, context: GenerationContext, files_written: List[str]
    ):
        """Commit generated files to git."""
        print("\nğŸ’¾ Committing to git...")
        frs_id = self._extract_frs_id(context.frs.title)

        from .tools import commit_changes

        commit_result = commit_changes(
            f"spec({frs_id}): add {context.service_type.value} specs via Agentic Loop",
            files_written,
        )

        if commit_result.get("success"):
            print("âœ… Changes committed successfully")
        else:
            print(f"âš ï¸ Commit failed: {commit_result.get('error')}")

    def _generate_summary_report(
        self, iterations: int, converged: bool, quality_results: Dict[str, QualityScore]
    ) -> Dict[str, Any]:
        """Generate summary report of the generation process."""
        # Calculate average quality
        avg_quality = 0.0
        if quality_results:
            avg_quality = sum(s.overall for s in quality_results.values()) / len(
                quality_results
            )

        # Document states
        doc_states = {
            doc_type: bundle.state.value for doc_type, bundle in self.documents.items()
        }

        # Quality breakdown
        quality_breakdown = {
            doc_type: {
                "overall": score.overall,
                "completeness": score.completeness,
                "consistency": score.consistency,
                "clarity": score.clarity,
                "technical_accuracy": score.technical_accuracy,
            }
            for doc_type, score in quality_results.items()
        }

        return {
            "iterations": iterations,
            "converged": converged,
            "average_quality": avg_quality,
            "document_states": doc_states,
            "quality_breakdown": quality_breakdown,
            "timestamp": datetime.now().isoformat(),
        }

    # Utility methods

    def _extract_frs_id(self, frs_path: str) -> str:
        """Extract FRS ID from file path."""
        path = Path(frs_path)
        match = re.search(r"FRS-(\d+)", str(path))
        return f"FRS-{match.group(1)}" if match else path.stem

    def _write_document(
        self, output_dir: str, filename: str, content: str
    ) -> Optional[str]:
        """Write document content to file."""
        try:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)

            file_path = output_path / filename
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content)

            return str(file_path)
        except Exception as e:
            print(f"âŒ Failed to write {filename}: {str(e)}")
            return None

    def _clean_agent_response(self, response: str) -> str:
        """Clean agent response by removing wrappers."""
        if not response:
            return ""

        # Remove markdown code blocks
        response = re.sub(
            r"```(?:markdown|md)?\n(.*?)\n```", r"\1", response, flags=re.DOTALL
        )

        # Remove leading explanations
        lines = response.split("\n")
        for i, line in enumerate(lines):
            if line.startswith("#"):
                return "\n".join(lines[i:])

        return response.strip()

    def _clean_json_response(self, response: str) -> str:
        """Clean JSON response from agent."""
        # Remove code blocks
        if "```json" in response:
            match = re.search(r"```json\s*\n(.*?)\n```", response, re.DOTALL)
            if match:
                return match.group(1).strip()

        # Find JSON object
        json_start = response.find("{")
        if json_start != -1:
            json_end = response.rfind("}")
            if json_end > json_start:
                return response[json_start : json_end + 1]

        return response.strip()

    async def _save_document_immediately(
        self, context: GenerationContext, doc_type: str, bundle: DocumentBundle
    ):
        """Save document immediately to disk with progress feedback."""
        try:
            # Determine filename
            filename = f"{doc_type}.md"
            content = bundle.content

            # Handle OpenAPI special case with enhanced error handling
            if doc_type == "openapi" and bundle.content:
                try:
                    # Check content size before conversion
                    if len(bundle.content) > 8000:
                        print(
                            f"    âš ï¸ OpenAPI content too large ({len(bundle.content)} chars), keeping as markdown"
                        )
                    else:
                        # Try to convert to JSON with minimal prompt
                        json_prompt = f"Convert to JSON:\n\n{bundle.content[:4000]}"
                        json_result = self.agents["md_to_json"](json_prompt)
                        json_content = self._clean_json_response(str(json_result))

                        # Validate JSON
                        import json

                        parsed = json.loads(json_content)

                        # Ensure it's not too large
                        if len(json_content) < 15000:  # 15KB limit
                            filename = "apis.json"
                            content = json_content
                            print(
                                f"    âœ… Successfully converted to JSON ({len(json_content)} chars)"
                            )
                        else:
                            print(
                                f"    âš ï¸ JSON too large ({len(json_content)} chars), keeping as markdown"
                            )

                except Exception as e:
                    print(
                        f"    âš ï¸ JSON conversion failed ({type(e).__name__}): keeping as markdown"
                    )
                    # Keep as markdown if JSON conversion fails

            # Write file
            file_path = self._write_document(context.output_dir, filename, content)
            if file_path:
                elapsed = time.time() - self.start_time
                self.last_save_times[doc_type] = time.time()
                print(
                    f"    ğŸ’¾ Saved {filename} (quality: {bundle.quality_score.overall:.1f}%, elapsed: {elapsed:.1f}s)"
                )
                return file_path
        except Exception as e:
            print(f"    âš ï¸ Failed to save {doc_type}: {str(e)}")
        return None

    def _should_continue_iteration(
        self,
        previous_scores: Dict[str, float],
        current_results: Dict[str, QualityScore],
    ) -> bool:
        """Check if iteration should continue based on improvement threshold."""
        meaningful_improvements = 0
        total_docs = len(current_results)

        for doc_type, current_score in current_results.items():
            previous_score = previous_scores.get(doc_type, 0.0)
            improvement = current_score.overall - previous_score

            if improvement >= self.config.min_improvement_threshold:
                meaningful_improvements += 1
                print(
                    f"    ğŸ“ˆ {doc_type}: {previous_score:.1f}% â†’ {current_score.overall:.1f}% (+{improvement:.1f}%)"
                )
            else:
                print(
                    f"    ğŸ“Š {doc_type}: {previous_score:.1f}% â†’ {current_score.overall:.1f}% (+{improvement:.1f}%)"
                )

        # Continue if at least 50% of documents show meaningful improvement
        should_continue = meaningful_improvements >= (total_docs * 0.5)
        print(
            f"    ğŸ¯ Meaningful improvements: {meaningful_improvements}/{total_docs} (threshold: {self.config.min_improvement_threshold}%)"
        )

        return should_continue

    def _extract_api_essentials(self, content: str) -> str:
        """Extract essential API information from content, keeping it concise."""
        if not content:
            return "No specific information available"

        # Extract only lines mentioning API, endpoints, authentication, or data
        essential_lines = []
        lines = content.split("\n")

        keywords = [
            "api",
            "endpoint",
            "auth",
            "login",
            "user",
            "data",
            "response",
            "request",
            "get",
            "post",
            "put",
            "delete",
        ]

        for line in lines[:50]:  # Only check first 50 lines
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in keywords):
                essential_lines.append(line.strip())
                if len(essential_lines) >= 10:  # Limit to 10 most relevant lines
                    break

        result = "\n".join(essential_lines)
        return (
            result[:800]
            if result
            else "Core API functionality with standard CRUD operations"
        )

    def _generate_minimal_openapi(self, context: GenerationContext) -> str:
        """í† í° ì œí•œì— ë‹¬í–ˆì„ ë•Œ ìµœì†Œí•œì˜ OpenAPI ìƒì„±."""
        from datetime import datetime

        service_name = context.frs.title.replace("FRS ", "").replace("-", " ")

        return f"""# {service_name} API

## API ì •ë³´
- **ì œëª©**: {service_name} API
- **ë²„ì „**: 1.0.0
- **ì„¤ëª…**: ìµœì†Œí•œ API ëª…ì„¸

## ì¸ì¦
- **ìœ í˜•**: Bearer í† í°
- **í—¤ë”**: Authorization: Bearer <token>

## í•µì‹¬ ì—”ë“œí¬ì¸íŠ¸

### í—¬ìŠ¤ ì²´í¬
- **GET** `/health`
- **ì„¤ëª…**: API ìƒíƒœ í™•ì¸
- **ì‘ë‹µ**: 200 OK

### ì¸ì¦
- **POST** `/auth/login`
- **ì„¤ëª…**: ì‚¬ìš©ì ë¡œê·¸ì¸
- **ìš”ì²­**: {{"email": "string", "password": "string"}}
- **ì‘ë‹µ**: 200 OK with token

### ë©”ì¸ ë¦¬ì†ŒìŠ¤
- **GET** `/api/v1/items`
- **ì„¤ëª…**: ì•„ì´í…œ ëª©ë¡ ì¡°íšŒ
- **ì‘ë‹µ**: 200 OK with array

- **POST** `/api/v1/items`
- **ì„¤ëª…**: ì•„ì´í…œ ìƒì„±
- **ìš”ì²­**: {{"name": "string", "data": "object"}}
- **ì‘ë‹µ**: 201 Created

ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

    def _generate_fallback_openapi(self, context: GenerationContext) -> str:
        """í´ë°± OpenAPI ë¬¸ì„œ ìƒì„±."""
        from datetime import datetime

        return f"""# OpenAPI ë¬¸ì„œ

## API ì •ë³´
- ì œëª©: {context.frs.title} API
- ë²„ì „: 1.0.0
- ì„œë¹„ìŠ¤ ìœ í˜•: {context.service_type.value}

## ê¸°ë³¸ URL
- ê°œë°œ: http://localhost:3000/api/v1
- ìš´ì˜: https://api.example.com/v1

## ì¸ì¦
- ìœ í˜•: Bearer í† í° (JWT)
- í—¤ë”: Authorization: Bearer <token>

## ê³µí†µ ì—”ë“œí¬ì¸íŠ¸

### í—¬ìŠ¤ ì²´í¬
- **GET** `/health`
- ì„¤ëª…: ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
- ì‘ë‹µ: 200 OK

### ì¸ì¦
- **POST** `/auth/login`
- ì„¤ëª…: ì‚¬ìš©ì ì¸ì¦
- ìš”ì²­ ë°”ë””: {{"email": "string", "password": "string"}}
- ì‘ë‹µ: 200 OK with JWT token

### ì‚¬ìš©ì ê´€ë¦¬
- **GET** `/users/profile`
- ì„¤ëª…: ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ
- í—¤ë”: ì¸ì¦ í•„ìˆ˜
- ì‘ë‹µ: 200 OK with user data

ìƒì„± ëŒ€ìƒ: {context.frs.title}
ë¶„ì„ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

    def _generate_fallback_requirements(self, context: GenerationContext) -> str:
        """í´ë°± ìš”êµ¬ì‚¬í•­ ë¬¸ì„œ ìƒì„±."""
        from datetime import datetime

        return f"""# ìš”êµ¬ì‚¬í•­ ë¬¸ì„œ

## í—¤ë”/ë©”íƒ€
- ë¬¸ì„œ: {context.frs.title} ìš”êµ¬ì‚¬í•­
- ì„œë¹„ìŠ¤ ìœ í˜•: {context.service_type.value}
- ìƒì„±ì¼: {datetime.now().isoformat()}

## ë²”ìœ„
- ì„œë¹„ìŠ¤ ê²½ê³„ ë° ì¸í„°í˜ì´ìŠ¤
- í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„

## ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­
- REQ-001: í•µì‹¬ ì„œë¹„ìŠ¤ ê¸°ëŠ¥ êµ¬í˜„
- REQ-002: ì‚¬ìš©ì ì¸ì¦ ë° ê¶Œí•œ ë¶€ì—¬
- REQ-003: ë°ì´í„° ê²€ì¦ ë° ì²˜ë¦¬

## ì˜¤ë¥˜ ìš”êµ¬ì‚¬í•­
- ERR-001: ì˜ëª»ëœ ì…ë ¥ì„ ìš°ì•„í•˜ê²Œ ì²˜ë¦¬
- ERR-002: ë””ë²„ê¹…ì„ ìœ„í•´ ëª¨ë“  ì˜¤ë¥˜ ë¡œê¹…
- ERR-003: ì˜ë¯¸ ìˆëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ

## ë³´ì•ˆ & ê°œì¸ì •ë³´
- SEC-001: ë³´ì•ˆ ì¸ì¦
- SEC-002: ì €ì¥ ë° ì „ì†¡ ì‹œ ë°ì´í„° ì•”í˜¸í™”
- SEC-003: GDPR ì¤€ìˆ˜

## ê´€ì¸¡ ê°€ëŠ¥ì„±
- OBS-001: í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
- OBS-002: ì„±ëŠ¥ ë©”íŠ¸ë¦­
- OBS-003: ì˜¤ë¥˜ ì¶”ì 

## ìˆ˜ìš© ê¸°ì¤€
- ëª¨ë“  ìš”êµ¬ì‚¬í•­ì´ êµ¬í˜„ë˜ê³  í…ŒìŠ¤íŠ¸ë¨
- ë³´ì•ˆ ì¡°ì¹˜ê°€ ì œìë¦¬ì— ìˆìŒ
- ì„±ëŠ¥ ëª©í‘œê°€ ë‹¬ì„±ë¨
"""

    def _generate_fallback_design(self, context: GenerationContext) -> str:
        """í´ë°± ì„¤ê³„ ë¬¸ì„œ ìƒì„±."""
        from datetime import datetime

        return f"""# ì„¤ê³„ ë¬¸ì„œ

## ì•„í‚¤í…ì²˜
- ì„œë¹„ìŠ¤ ì§€í–¥ ì•„í‚¤í…ì²˜
- RESTful API ì„¤ê³„
- ë°ì´í„°ë² ì´ìŠ¤ ì§€ì†ì„± ê³„ì¸µ

## ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨
```mermaid
sequenceDiagram
    í´ë¼ì´ì–¸íŠ¸->>ì„œë¹„ìŠ¤: ìš”ì²­
    ì„œë¹„ìŠ¤->>ë°ì´í„°ë² ì´ìŠ¤: ì¿¼ë¦¬
    ë°ì´í„°ë² ì´ìŠ¤-->>ì„œë¹„ìŠ¤: ì‘ë‹µ
    ì„œë¹„ìŠ¤-->>í´ë¼ì´ì–¸íŠ¸: ê²°ê³¼
```

## ë°ì´í„° ëª¨ë¸
- ì‚¬ìš©ì ì—”í‹°í‹°
- ì„¸ì…˜ ê´€ë¦¬
- ê°ì‚¬ ë¡œê¹…

## API ê³„ì•½
- í‘œì¤€ REST ì—”ë“œí¬ì¸íŠ¸
- JSON ìš”ì²­/ì‘ë‹µ í˜•ì‹
- HTTP ìƒíƒœ ì½”ë“œ

## ë³´ì•ˆ & ê¶Œí•œ
- JWT ì¸ì¦
- ì—­í•  ê¸°ë°˜ ì ‘ê·¼ ì œì–´
- API ìš”ì²­ ë¹ˆë„ ì œí•œ

## ì„±ëŠ¥ ëª©í‘œ
- ì‘ë‹µ ì‹œê°„ < 200ms
- 99.9% ê°€ìš©ì„±
- ìˆ˜í‰ í™•ì¥ì„±
"""

    def _generate_fallback_tasks(self, context: GenerationContext) -> str:
        """í´ë°± ì‘ì—… ë¬¸ì„œ ìƒì„±."""
        from datetime import datetime

        return f"""ì‘ì—… ë¬¸ì„œ

## ì—í”½
| ì—í”½ ID | ì œëª© | ì„¤ëª… |
|---------|-------|-------------|
| E-001   | ì„œë¹„ìŠ¤ êµ¬í˜„ | í•µì‹¬ ì„œë¹„ìŠ¤ ê°œë°œ |

## ìŠ¤í† ë¦¬
| ìŠ¤í† ë¦¬ ID | ì—í”½ ID | ì œëª© | ìš°ì„ ìˆœìœ„ |
|----------|---------|-------|----------|
| S-001    | E-001   | í•µì‹¬ API | ë†’ìŒ |
| S-002    | E-001   | ë³´ì•ˆ | ë†’ìŒ |

## íƒœìŠ¤í¬
| íƒœìŠ¤í¬ ID | ìŠ¤í† ë¦¬ ID | ì œëª© | ì˜ˆìƒì‹œê°„ |
|---------|----------|-------|----------|
| T-001   | S-001    | ì„¤ì • | 4h |
| T-002   | S-001    | êµ¬í˜„ | 16h |

## DoD (ì™„ë£Œ ì •ì˜)
- [ ] ì½”ë“œ ì™„ì„±
- [ ] í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ
- [ ] ë¬¸ì„œí™” ì™„ë£Œ
"""

    def _generate_fallback_changes(self, context: GenerationContext) -> str:
        """í´ë°± ë³€ê²½ì‚¬í•­ ë¬¸ì„œ ìƒì„±."""
        from datetime import datetime

        return f"""ë³€ê²½ì‚¬í•­ ë¬¸ì„œ

## ë²„ì „ ì´ë ¥
| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ì‚¬í•­ |
|---------|------|---------|
| 1.0.0   | {datetime.now().strftime('%Y-%m-%d')} | ì´ˆê¸° ë¦´ë¦¬ìŠ¤ |

## ë³€ê²½ ìš”ì•½
- ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ êµ¬í˜„
- í•µì‹¬ ê¸°ëŠ¥ ë°°í¬

## ì˜í–¥/ìœ„í—˜
- ì˜í–¥: ìƒˆë¡œìš´ ì„œë¹„ìŠ¤
- ìœ„í—˜: ë³´í†µ
- ì™„í™”: í…ŒìŠ¤íŠ¸ ë° ëª¨ë‹ˆí„°ë§

## ë¡¤ë°± ê³„íš
1. ì„œë¹„ìŠ¤ ì¤‘ë‹¨
2. ë°°í¬ ë¡¤ë°±
3. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸

## ì•Œë ¤ì§„ ë¬¸ì œ
- ì´ˆê¸° ë°°í¬
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""

    def _parse_quality_score(self, result: str) -> QualityScore:
        """ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ í’ˆì§ˆ ì ìˆ˜ íŒŒì‹±."""
        score = QualityScore()

        # Simple pattern matching for scores
        patterns = {
            "completeness": r"completeness[:\s]+(\d+)",
            "consistency": r"consistency[:\s]+(\d+)",
            "clarity": r"clarity[:\s]+(\d+)",
            "technical_accuracy": r"technical[:\s]+accuracy[:\s]+(\d+)",
        }

        for attr, pattern in patterns.items():
            match = re.search(pattern, result, re.IGNORECASE)
            if match:
                setattr(score, attr, float(match.group(1)))

        # Extract feedback
        feedback_match = re.search(
            r"feedback:(.*?)(?:$|\n\n)", result, re.DOTALL | re.IGNORECASE
        )
        if feedback_match:
            feedback_text = feedback_match.group(1).strip()
            score.feedback = [f.strip() for f in feedback_text.split("\n") if f.strip()]

        score.calculate_overall()
        return score

    def _parse_consistency_issues(self, result: str) -> List[str]:
        """ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ ì¼ê´€ì„± ë¬¸ì œ íŒŒì‹±."""
        issues = []

        # Look for bullet points or numbered lists
        lines = result.split("\n")
        for line in lines:
            line = line.strip()
            if line and (
                line.startswith("-")
                or line.startswith("*")
                or re.match(r"^\d+\.", line)
            ):
                # Remove list markers
                issue = re.sub(r"^[-*\d.]+\s*", "", line)
                if issue:
                    issues.append(issue)

        return issues


# Convenience function for synchronous usage
def generate_specs_agentic(
    frs_path: str,
    service_type: ServiceType,
    output_dir: Optional[str] = None,
    config: Optional[Config] = None,
) -> Dict[str, Any]:
    """
    Synchronous wrapper for Agentic Loop specification generation.

    Args:
        frs_path: Path to FRS markdown file
        service_type: Type of service (API or WEB)
        output_dir: Custom output directory
        config: Configuration object

    Returns:
        Generation results with quality metrics
    """
    orchestrator = AgenticOrchestrator(config)

    # Run async method in sync context
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        result = loop.run_until_complete(
            orchestrator.generate_specs_with_loop(
                frs_path=frs_path, service_type=service_type, output_dir=output_dir
            )
        )
        return result
    finally:
        loop.close()
